{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- save and load as  classmethod?\n",
    "- overload constructor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/shakespeare.txt') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import FunctionTransformer\n",
    "# from unidecode import unidecode\n",
    "# from tokenizer.bpe_tokenizer import BytePairEncodingTokenizer\n",
    "\n",
    "# tokenizer = BytePairEncodingTokenizer(0)\n",
    "# tokenizer.load('tokenizer/trained_tokenizers/bpe.pkl')\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('unicode_to_ascii', FunctionTransformer(unidecode)),\n",
    "#     ('encode', FunctionTransformer(tokenizer.encode))\n",
    "# ])\n",
    "\n",
    "# pipeline.transform('lowest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1095\n"
     ]
    }
   ],
   "source": [
    "from tokenizer.bpe_tokenizer import BytePairEncodingTokenizer\n",
    "\n",
    "text = text[:50000]\n",
    "\n",
    "tokenizer = BytePairEncodingTokenizer(1000)\n",
    "tokenizer.fit([text])\n",
    "\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 64\n",
    "batch_size = 64\n",
    "\n",
    "def get_batch():\n",
    "    idx = torch.randint(len(data) - context_length, (batch_size, ))\n",
    "    x = torch.stack([data[i:i + context_length] for i in idx])\n",
    "    y = torch.stack([data[i + 1:i + context_length + 1] for i in idx])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.061255 M parameters\n"
     ]
    }
   ],
   "source": [
    "from language_model.model import Encoder\n",
    "\n",
    "embed_size = 512\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "forward_expansion = 4\n",
    "dropout = 0\n",
    "\n",
    "model = Encoder(\n",
    "    tokenizer.vocab_size,\n",
    "    embed_size,\n",
    "    context_length,\n",
    "    num_layers,\n",
    "    num_heads,\n",
    "    forward_expansion,\n",
    "    dropout\n",
    ")\n",
    "\n",
    "print(sum([p.numel() for p in model.parameters()]) / 1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      " arth\n",
      "seCw show thee T\n",
      "\n",
      ";esAu F\n",
      " keepthe atstononinthth pte'd,ionnot:,\n",
      "VIRGILIA:hreow\n",
      "c thedcome \n",
      "reodw:adoorlC\n",
      "Il\n",
      "Marciushus ,ghepthe  mo ueyeedaamfolethey le AUFIDIUS:heacavecabon wcome\n",
      "elseesgo c er aLARTIUS: edearwre  brweattheve ,neandthate  nean batfiseu\n",
      "tonAflya \n",
      ";theersfa\n",
      "rtheMes\n",
      " a -ctiou gm noblesay atc   am\n",
      "hath\n",
      " carry ahuhm ati , sayinof My\n",
      "VOLUMNIA:WCome,he Firstuswellpeople,Ion.'comehMARCIUS:vatrs!con\n",
      " madam;is  \n",
      "conitiMarciusIarsNthata Marcius ;\n",
      "'tmy herf poorUSinut\n",
      "we fe'Tishe inionWisyou\n",
      "ow: LARTIUS:wis ding\n",
      "matter isMaredreo' a atisirc leleinlebes\n",
      "havepo awwhich\n",
      "ouWld rnans  vnisandased, erisour hekbutfemathiw\n",
      "  the on umbutr'tuegtheube do \n",
      "mionno ce\n",
      "worthyto IbonkambulesTend ones\n",
      "seels!sword to\n",
      "aeny. go\n",
      "liat.roer atyatHa\n",
      "restalreadHowesupsuf  es orittimealove ofbe     welldasdo ithethlne ap\n",
      " have  itfahem,enourfiwinonomEvenld:thesayconrihanots. s  \n",
      "enLARTIUS: Ofwillswethetledmight   th  .l s we tiyy   \n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.05)\n",
    "\n",
    "for iter in range(50):\n",
    "    if iter % 5 == 0:\n",
    "        print(iter)\n",
    "\n",
    "    x_batch, y_batch = get_batch()\n",
    "\n",
    "    logits, loss = model(x_batch, y_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long)\n",
    "print(tokenizer.decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
