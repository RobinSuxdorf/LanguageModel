{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/shakespeare.txt') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import bpe_tokenizer\n",
    "from pipelines import text_to_tensor\n",
    "\n",
    "text = text[:10000]\n",
    "\n",
    "tokenizer = bpe_tokenizer.BytePairEncodingTokenizer(250)\n",
    "tokenizer.fit([text])\n",
    "\n",
    "train_data = text[:9000]\n",
    "test_data = text[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from language_model import generation\n",
    "\n",
    "model = generation.LanguageModel(tokenizer, device)\n",
    "\n",
    "print(sum([p.numel() for p in model.encoder.parameters()]) / 1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(' ', max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from language_model import train\n",
    "\n",
    "trainer = train.ModelTrainer(model, train_data, test_data)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(' ', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from itertools import chain\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "\n",
    "def get_wikipedia_data(link: str):\n",
    "    try:\n",
    "        page = wikipedia.page(link)\n",
    "\n",
    "        page_links = page.links\n",
    "\n",
    "        data = {\n",
    "            'title': page.title,\n",
    "            'summary': page.summary\n",
    "        }\n",
    "\n",
    "        return data, page_links\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {link}: {e}\")\n",
    "        return None, []\n",
    "\n",
    "n_pages = 1000\n",
    "sample_size = 500\n",
    "all_links = {'Deep Learning'}\n",
    "visited_links = set()\n",
    "data = []\n",
    "\n",
    "# all_links empty case\n",
    "# duplicated keys\n",
    "# generation: maximum token length\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=sample_size) as executor:\n",
    "    while len(data) < n_pages and len(all_links) > 0:\n",
    "        current_sample_size = min(sample_size, len(all_links), n_pages - len(data))\n",
    "\n",
    "        link_sample = random.sample(list(all_links), current_sample_size)\n",
    "\n",
    "        visited_links.update(link_sample)\n",
    "\n",
    "        for link in link_sample:\n",
    "            all_links.remove(link)\n",
    "\n",
    "        futures = [executor.submit(get_wikipedia_data, link) for link in link_sample]\n",
    "        for future in futures:\n",
    "            page_data, links = future.result()\n",
    "            if page_data:\n",
    "                data.append(page_data)\n",
    "\n",
    "                new_links = {link for link in links if link not in visited_links}\n",
    "\n",
    "                all_links.update(new_links)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df.groupby('title').size()\n",
    "\n",
    "s[s > 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
